[
  {
    "page": 1,
    "bbox": null,
    "text": "Automated Essay Scoring Using Transformer Models\n\nSabrina Ludwig 1*, Christian Mayer 1, Christopher Hansen2, Kerstin Eilers?, and Steffen Brandt 4\n\nUniversity of Mannheim; sabrina.ludwig@uni-mannheim.de; mayer@bwl.uni-mannheim.de\nKiel, Germany; info@christopher-hansen.de\n\nKiel, Germany; kerstin_eilers@gmx.de\n\nopencampus.sh; steffen@opencampus.sh\n\nCorrespondence: sabrina.ludwig@uni-mannheim.de\n\n* PB OW N\n\nAbstract:\n\nAutomated essay scoring (AES) is gaining increasing attention in the education sector as it significantly reduces the burden\nof manual scoring and allows ad hoc feedback for learners. Natural language processing based on machine learning has\nbeen shown to be particularly suitable for text classification and AES. While many machine-learning approaches for AES\nstill rely on a bag-of-words (BOW) approach, we consider a transformer-based approach in this paper, compare its\nperformance to a logistic regression model based on the BOW approach and discuss their differences. The analysis is based\non 2,088 email responses to a problem-solving task, that were manually labeled in terms of politeness. Both transformer\nmodels considered in that analysis outperformed without any hyper-parameter tuning the regression-based model. We\nargue that for AES tasks such as politeness classification, the transformer-based approach has significant advantages, while\na BOW approach suffers from not taking word order into account and reducing the words to their stem. Further, we show\nhow such models can help increase the accuracy of human raters, and we provide a detailed instruction on how to\nimplement transformer-based models for one’s own purposes.\n\nKeywords: automatic essay scoring; natural language processing; machine learning; logistic regression; transformer models;\nassessment; R; Python\n\nContents\nL. Introduction ........ccccccesessssssssssesesessssssesesssesesessssenenesesesesesesessenesesesesesessssenesesesesesesesesssenesesesesesesessenesesesesesesesseneaeseaeaeseneeses 2\n2. Methodological Background for Automated Essay Scoring and NLP based on Neural Networks .............. 2\n2.1 Traditional Approaches ..ccccccccccccccscssscscssesssesesceseseseseeesesesesesessee ses esesesesesssesssesssesssssssseseesesesssssssssssssesssesssesseseseseseseses 2\n2.2 Approaches Based on Neural Networks ..c.ccccccccccseeecscecesesessesssessseesessssssssssssssssssssssessnssssssssssssssssssssssisssseseseseseses 3\n2.2.1 Terms and General Methodological Background... sees ese eseseeesesesesesesesesssssssenesessescsesesessssseeeenees 3\n2.2.2 Methodological Background on Recurrent Neural Nets (RNN) ....cccccscsesssseseseseeeeeeeenesesesesseeeneneeneseseeeeeenes 4\n2.2.3 Results of Recurrent Neural Networks for Automated Essay Scoring Tasks ........cccseeseeeeseneteeseeeeeeees 4\n2.2.4 Methodological Background on Transformer Models ..........ccccceeseeeeeseseesesesesssscseesesessssesesesesssssseseenees 5\n2.2.5 Results of Transformer Models for Automated Essay Scoring Tasks .........cccsccseseeseteesesseeeseneneeseseeeeeenes 7\n3. Data.....cccscsssssssssssssssrsscnsnenesesesessssssssssesssesesesesesesessssesenensseneneseseseseaeeneneneneaeseseseseassneseseseseseseaeaeaesesseseneneneaeneneseseseseseesenensnses 7\nA. Method. .........sccececssssssssesesessssssesescscscscssssssesesesenesessesenenesesesesessssenesesesesesesesesssneseseseseseseeseneaeseseseseseseaesesesenesesseneneneaeaeseneeees 8\n4.1 Basic Data Preparation Used in Both Approaches...ccccccccccccscssscssssssesscsssssssssssssssssssnssssssssssssssssesssesssssssesesesesenes 8\n4.2 Regression Model Estimation ...c.csccccccssssssssssssssssesesssssnsnessesesessssenesssssesssesescsssnesssssssssesesesnensnssssesescsssnenenssssssssessenentiens 9\n4.3 Transformer-Based C1ASSification .....scscccscesesssssesescssssssetssssesesssssnessssssssesessensessensnssssesesessenssnesssssesesssnenensnsissssessenensnens 10\n5. Resullts........ccscccsessssssssssssesesessssesesesesesesesesssssesesesesesessssenesesesesesesesssnesesenesesesesesessenesesesesenesesseseneseaeseseseseeseaesesesenesesseseneaeas 12\n6. DISCUSSION ........scscsssesesessssssesesesesessssssesssesenesessssenesesesesesesesssesesesesesesssseneneaesesesenesesecsenesesesesesesesessesesesesesesesseneneneseacseseeees 13\n7. CONCIUSION........scssscseessssssessescsesessssssesssesenesssssseseaesesesesesesssnesesesesesesssseneaeaesesesenenesesseneneaeaesesesessesesesesesenesesscneneneseacseseeees 14\n\nReferences ...........csscccsssccsscsesccsseccsscccesscecsceessecessceessecesseeesseceescceesseeescecesseesssseceseessaeseesseeseeeessceesaeeesseeesacecsseeesaeeeesseesaseeeseees 15",
    "type": "text"
  },
  {
    "page": 2,
    "bbox": null,
    "text": "1. Introduction\n\nRecent developments in Natural Language Processing (NLP) and the progress in Machine Learning (ML)\nalgorithms have opened the door to new approaches within the educational sector in general and the\nmeasurement of student performance in particular. Intelligent tutoring systems, plagiarism-detecting software,\nor helpful chatbots are just a few examples of how ML is currently used to support learners and teachers [1].\nAn important part of providing personalized feedback and supporting students is automated essay scoring\n(AES), in which algorithms are implemented to classify long text answers in accordance with classifications by\nhuman raters [2]. In this paper, we implement AES using current state-of-the-art language models based on\nneural networks with a transformer architecture [3,4]. In doing so, we want to explore the following two main\nquestions:\n\n(1) To what extent does a transformer-based NLP model produce benefits compared to a traditional\nregression-based approach for AES?\n\n(2) In which ways can transformer-based AES be used to increase the accuracy of scores by human\nraters?\n\nIn a first step, we provide information on current work in the field of AES and the different methods of\nconducting automated essay scoring. Since transformer models are a very promising development, we further\nprovide a more detailed explanation on their basic characteristics and their differences from the approaches\nused previously. By providing the most relevant technical details for the implementation, we also aim to\nprovide practical guidance for the use of transformer-based models for one’s own purposes. We then present\nthe results of an empirical example based on student responses to a problem-solving task submitted as an email\nin a simulated office environment. Finally, we will conclude with a discussion of the formulated questions, the\nfuture relevance of transformer-based models in AES and their limitations.\n\n2. Methodological Background for Automated Essay Scoring and NLP based on Neural Networks\n2.1 Traditional Approaches\n\nAutomated essay scoring has a long history. In the early 1960s, the Project Essay Grade system (PEG), as\none of the first automated essay scoring systems was developed by Page [5-7]. In the first attempts, four raters\ndefined criteria (proxy variables) while assessing 276 essays in English by 8th to 12th graders. The system uses\na standard multiple regression analysis involving the defined proxy variables (text features such as document\nlength, grammar, or punctuation) representing the independent variables with the human-rated essay score\nbeing the dependent variable [8]. With a multiple correlation coefficient of .71 as an overall accuracy for both\nthe training and test set and given the good prediction of the human score for the test set based on derived\nweights from the training set, the computer reached comparable results to humans [7]. However, with a\ncorrelation of .51 for the average document length [7,8] the PEG could be easily manipulated by writing longer\ntexts [2,9]. Thus, this kind of score prediction considers only surface aspects and ignores semantics and the\ncontent of the essays [8].\n\nIn subsequent studies, the n-gram model based on the bag-of-words approach (BOW) was commonly used\nfor a number of decades (e.g.[10-13]). BOW models extract features from student essays, in the case of the n-\ngram model, by counting the occurrences of terms consisting of n words. They then consider the number of\nshared terms between essays of the same class and model their relationship [10,14]. An often-cited AES system\nusing BOW is the Electronic Essay Rater (e-rater for short) developed by [15]. The e-rater predicts scores for\nessay responses and was originally applied to responses written by non-native English speakers taking the\nGraduate Management Admission Test (GMAT; 13 considered questions) and the Test of English as a Foreign\nLanguage (TOEFL; two considered questions). Two features used in the e-rater are extracted by content vector\nanalysis programs named EssayContent and ArgContent. While the former uses a BOW approach on the full\nresponse, the latter (ArgContent) uses a weighted BOW approach for each required argument in a response.\nUsing these features alone results in average accuracies of .69 and .82, respectively. Including all 57 predictive\nfeatures of the e-rater, the accuracy ranges from .87 to .94 (the number of responses varies between 260 and 915\nfor each question with a mean of 638; a Kappa value is not reported).\n\nThe above models are based on laborious, theory-based feature extraction methods. Therefore, methods\nsuch as Latent Semantic Analysis (LSA) were established [16]. LSA are corpora-specific and trained on texts",
    "type": "text"
  },
  {
    "page": 3,
    "bbox": null,
    "text": "that are related to the given essay topic. They provide a semantic representation of an essay, which is then\ncompared to the semantic representation of other similarly scored responses. In this way a feature vector, or\nembedding vector, for an essay is constructed, which is then used to predict the score. Foltz, Laham, and\nLandauer [17] used LSA in the Intelligent Essay Assessor system and compared the accuracy of the system\nagainst two human raters. The analysis is based on 1,205 essay answers on 12 diverse topics given in the GMAT.\nThe system achieves a correlation of .70 with the human rating, while the two human ratings show a correlation\nof .71 (a Kappa value was not reported).\n\n2.2 Approaches Based on Neural Networks\n\nThe current possibilities for automated essay scoring are largely influenced by the advances in NLP based\non neural networks. In this section, we therefore first want to provide a short overview of the dynamic\ndevelopment in this area in the past few years. Since the terminology used in the machine learning context is\noften different from that used in psychometrics, and is also sometimes ambiguous, beforehand some initial\ncorresponding comments on the meaning of some of the terms used and some additional methodological\nbackground will be provided. Thereafter, more detailed background on two important NLP approaches based\non neural networks are given along with their corresponding achievements in AES.\n\n2.2.1 Terms and General Methodological Background\n\nModel\n\nModel is an ambiguous term in the context of machine learning. On the one hand, it can merely refer to a\nparticular architecture for a machine learning model; on the other hand, it can also refer to a model architecture\nthat has already been pre-trained with a specific training dataset, that is, that contains the model weights from\nthe pre-training. Therefore, unless it is clear from the context, we will use the terms architecture or checkpoint!,\nrespectively, instead of the term model.\n\nUnless it is clear due to the context, we will therefore use the terms architecture, or checkpoint, respectively,\ninstead of the term model.\n\nToken/ Tokenization?\n\nThe initial input for an NLP task is usually a text sequence. The first step is to turn this text sequence into\na sequence of numbers, which is then interpretable for the neural networks. This is done by splitting the text\ninto different tokens, assigning each token a number, and storing the token-number relationship in a dictionary.\nThe corresponding process is called tokenization and can be word-based, character-based, or subword-based.\nWord-based tokenization is typically the starting point for the traditional approaches described above. To\nreduce the huge amount of possibly occurring words in a sentence, it is thereby usually necessary to reduce the\namount of variation in words by removing stop words or using a technique called stemming, which ignores\nconjugations, declinations, or plurals and only considers the stem of a word (see, for example, [20]).\nIn a character-based tokenization the text is split into characters rather than words. This approach has the\nadvantage that the dictionary of possible tokens is usually very small, and that there are far fewer out of\nvocabulary tokens in comparison to the word-based approach, where the dictionary must be restricted to a\ncertain maximum of words and where all other words will be tokenized as unknown or simply removed. The\ndisadvantage, however, is that each token only incorporates a very limited amount of information (for Chinese\nlanguages, for example, this is different though), and the tokenization of an input sequence will lead to a large\namount of input tokens for the model with a _ corresponding computational burden.\nSubword tokenization is a combination of these two approaches, in which words (sometimes even word pairs\nor groups) that occur very frequently are not split and other words are split into smaller subwords. For example,\nby splitting the word “clearly” into “clear” and “ly” or the word “doing” into “do” and “ing”. There are\ndifferent subword segmentation algorithms available for such tokenization. Popular ones are WordPiece [21],\nwhich is, for example, used in the BERT model [4], which we will apply for the essay scoring task demonstrated\n\n1 Checkpoint is a common term in computer science referring to a specific state of an application or program [18] and is used in\nthe same way for a given state of a trained model.\n2 This section is based substantially on remarks in the chapter Tokenization of the Hugging Face online course [19].",
    "type": "text"
  },
  {
    "page": 4,
    "bbox": null,
    "text": "in this paper; Byte-Pair-Encoding (BPE) [22] as used in the GPT-3 model [23]; or SentencePiece [24] or a unigram-\nbased approach [25], which are used in many multilingual models.\n\nPadding\n\nNeural networks exclusively work with input sequences of equal length. After tokenizing the input\nsequence, the second important step is therefore the padding, which yields an equal length of all input\nsequences. The length of the input sequence is given by the architecture of the used neural network that is used.\nThe padding step then either truncates each input sequence to a predefined length (if it is longer than the\npredefined length) or completes it with special empty tokens (if it is shorter than the predefined length).\n\nEmbeddings\n\nWhile the tokenization turns the text sequence into a sequence of numbers, the embeddings provide the\ntokens with a meaning by turning the single number into a high dimensional vector (in the case of the GPT-3,\nfor example, the embedding vector has a dimensionality of 12,288 [23]). The embedding vectors can be\ncalibrated either in a supervised way, by comparing each token’s meaning in relationship to a result for a\nparticular NLP task, or in an unsupervised way, by using token-to-token co-occurrence statistics (like, for\nexample, the GloVe embeddings [26] or by using a neural net with an unsupervised training task. A defining\ncharacteristic of current language models (see section below) and of their embeddings is how this unsupervised\ntraining task is defined. In general, though, one or several tokens are hidden, and the model —including the\nembeddings—is then trained (i.e., calibrated) to predict the hidden token. Besides the embeddings for the\nmeaning of a single tokens, transformer models additionally include embeddings for the position of a token\nwithin an input sequence, and sometimes also for segments of tokens, where the embeddings provide\ninformation on the probability that a certain series of tokens is followed by another series of tokens, or (in the\ncase of a supervised training) that a series of tokens has an equivalent meaning to another series of tokens [4,23].\n\nFeature\nA feature is an independent variable in an ML model. In an essay scoring task, the features typically consist\nof an embedding for each token of a text sequence.\n\nLabel\nThe dependent variable in an ML model is called a “label” while in an essay scoring task, this is usually\ncalled the “score”.\n\nLanguage Model\n\nIn NLP, a language model describes a model trained to predict the next word, subword, or character in a\ntext sequence. The best language models are currently all based on transformer architectures [27]. More details\non transformers are given in the corresponding chapter below.\n\n2.2.2 Methodological Background on Recurrent Neural Nets (RNN)*\n\nRNNs are a relatively old technique in terms of neural network structures. The two most prominent types\nof RNNs are the Gated Recurrent Units (GRU) [28] and the Long Short-term Memory (LSTM) [29]. The\ncorresponding models are largely responsible for breakthrough results in speech recognition and were also\ndominated the field of NLP until the appearance of transformer-based models. While the GRU is very efficient\nfor smaller datasets and shorter text sequences, the LSTM also allows longer text sequences to be modelled by\nintegrating a memory cell that can pass information relatively easily from one point to another within a\nsequence. In particular the LSTM thereby allowed the training of language models with contextual embeddings,\nand is embeddings where the position of a token in a sequence would make a difference for its meaning [30].\nDue to their sequential structure RNNs in general though have the disadvantage that training them is very\nslow. Furthermore, both GRU and LSTM suffer from the “vanishing gradient problem”, which describes an\noptimization problem that traditionally occurs in large neural nets and thereby makes the training of LSTMs\nfor very long sequences, for example, impractical [31].\n\n2.2.3 Results of Recurrent Neural Networks for Automated Essay Scoring Tasks\n\nFor a more in-depth introduction, we recommend Andrew Ng’s course on RNNs: https://www.coursera.org/lecture/nlp-\nsequence-models/recurrent-neural-network-model-ftkzt",
    "type": "text"
  },
  {
    "page": 5,
    "bbox": null,
    "text": "Table 1 provides exemplary studies examining similar tasks. The results for RNNs using the dataset\npublished in 2012 on Kaggle for the AES competition under the name “Automated Student Assessment Prize”\n(ASAP). Taghipour and Ng [32] applied RNNs based on self-trained word embeddings, and their combination\nof a CNN and a bidirectional LSTM model significantly outperforms the two baseline models, based on support\nvector regression and Bayesian linear ridge regression, while also outperforming models based on just the\nLSTM, the CNN, or a GRU.\n\nAlikaniotis and colleagues [33] also analyzed the abovementioned Kaggle dataset and propose a new, score\nspecific embedding approach in combination with a bidirectional LSTM yielding a quadratic weighted Kappa\nof .96. However, Mayfield and Black [34] are critical of this approach stating that it leads to skewed statistics by\ngrouping questions with maximum score ranging from 4 point to 60 points into a single dataset.\n\nTable 1. Exemplary Studies on Recurrent Neural Networks for Automated Essay Scoring Tasks\n\nStudy Task Data Model Kappa\nTaghipour & Scoring essay 12,978 essays LSTM + CNN 76\nNg (2016) answers to 8 with a length of\ndifferent 150 to 550 words\nquestions, some\nof which depend\nupon source\ninformation\nAlikaniotis et Scoring essay 12.978 essays LSTM 96\nal. (2016) answers to 8 with a length of combined\ndifferent 150 to 550 words with score-\nquestions, some specific word\nof which depend embeddings\n\nupon source\ninformation\n\n2.2.4 Methodological Background on Transformer Models\n\nFor the application of a transformer model, it is fundamental to understand the three basic architecture\ntypes of a transformer: encoder models, decoder models, and encoder-decoder models. Table 2 provides a brief\noverview of the three types, the tasks they are predominantly used for, and exemplary models that have been\nimplemented and trained based on the respective type.\n\nIn general, transformer models are neural networks based on the so-called attention mechanism and were\noriginally introduced in the context of language translation. The attention mechanism was presented in 2014 by\nBahdanau et al. [35]. They showed that instead of encoding a text from the source language into a vector\nrepresentation and then decoding the vector representation into the text of the target language, the attention\nmechanism allows to avoid this bottleneck of a vector representation to be avoided between the encoder and\ndecoder by allowing the model to directly search for relevant tokens in the source text when predicting the next\ntoken for the target text.\n\nIn 2017, Vaswani et al. [3] then showed that “Attention is All You Need”. While the encoder and decoder\nmodels for translation tasks before were mainly based on RNNs, the authors demonstrated that not only can\nthe described vector representation be replaced by an attention mechanism but the encoder and decoder models\nthemselves can be implemented based on the attention mechanism alone. They implemented a self-attention\nmechanism, in which different attention layers tell the model to focus on informative words and neglect\nirrelevant words. They showed that this way the model achieves new performance records on several\n\nFor a detailed introduction to the attention mechanism, we recommend Andrew Ng’s lecture on Attention Model Intuition:\nhttps://www.youtube.com/watch?v=SysgY ptB198",
    "type": "text"
  },
  {
    "page": 6,
    "bbox": null,
    "text": "translation benchmarks while having a fraction of the training cost compared to the best models previously\nused.5\n\nA major advantage of the transformer architecture is that it allows parallel processing of the input data and\nis not affected by the vanishing gradient problem. This makes it possible to train with larger and larger datasets,\nresulting in better and better language models [36]. Today, transformer-based architectures are applied to a\nlarge variety of tasks not only in NLP but also in fields like computer vision, predictions based on time series\nor tabular data, or multi-modal tasks [37-40]. And in the majority of fields transformer-based models are\ncurrently leading the benchmarks [42]. The additional capabilities of the model come with a price, however.\nWhile models pretrained with enormous datasets increase the performance, the computational processing of\nbillions of parameters is costly and time-consuming [43]. Moreover, it restricts the training of such models to\nlarge corporates or governmental institutions and could therefore prevent independent researchers or smaller\norganizations from gaining equivalent access to such models. [44].\n\nTable 2. Transformer Architectures\n\nArchitecture Examples Tasks\nALBERT, BERT, Sent lassification, named entit\nEncoder DistiIBERT, ELECTRA, sa “ - ty ne i\" . new an\nRoBERTa recognition, extractive question a ering\nCTRL, GPT, GPT-2,\nDecoder GPT-3, Transformer XL, Text generation\nGPT-J-6B, Codex\nBART, T5, Marian, Summarization, translation, generative question\nEncoder- decoder .\nmBART answering\n\nNote. Adapted from the “Hugging Face Course”, summary of chapter 1 (https://huggingface.co/course/chapter1/9).\n\nThe encoder-decoder architecture describes a sequence-to-sequence model as proposed in the original\n“Attention is All You Need” paper [3]. This type of architecture is particularly trained to transform (in the case\nof an NLP task) a text sequence of a certain format into a text sequence of another format, such as in translation\ntasks.\n\nThe encoder architecture includes (as indicated by the name) only the input, or left-hand side, of the\noriginal transformer architecture and transforms each input sequence into a numerical representation.\nTypically, encoder models like BERT, RoBERTa, or ALBERT are specially used for text classification or\nextractive question answering [4,45,46]. Since AES is a special case of text classification, this type of models is\ntherefore particularly well suited to AES, and accordingly the use of the BERT model has become increasingly\nprominent in the literature on AES [47].\n\nFinally, the decoder architecture includes only the output, or right-hand side, of the original transformer\nmodel. In the original model, the decoder takes the information of the input sequence from the encoder and\ngenerates in a stepwise procedure one token after the other for the output. At each step, the decoder not only\nconsiders the information from the decoder, but also combines it with the information given by the output\ntokens that were already generated. Using only the decoder architecture therefore results in a model that\ngenerates new tokens based on the tokens it already generated or based on a sequence of initial tokens it was\nprovided with (typically called “prompt”). Decoder models, like the GPT-3 or the Transformer XL, are therefore\napplied to generate text outputs. By providing specifically designed prompts such models can be used for other\ntasks such as translation or classification tasks as well though [48].\n\nAll mentioned models from the different architecture types have in common that they are, in a first step,\ntrained self-supervised on large text corpora to calibrate powerful language models. In a second step, they are\n\nFor a more thourough understanding, we recommend “The Illustrated Transformer” by Jay Alammar\n(https://jalammar.github.io/illustrated-transformer/) and the video “Transformer Neural Networks - Explained”\n(https://www.youtube.com/watch?v=TQQIZhbC5ps).\n\nFor an intutive comparison of the attention mechanism and RNNs, we further recommend: https://towardsdatascience.com/the-\nfall-of-rnn-lstm-2d1594c74ce0",
    "type": "text"
  },
  {
    "page": 7,
    "bbox": null,
    "text": "then fine-tuned to a specific task using supervised learning and hand-coded labels.* This way the language\nmodels can transfer the learned knowledge to any more specific task (transfer learning) [49].7\n\nThus, innovative approaches like high-dimensionality attention-based transformers moved into the focus\nas they process sequential data in parallel and are highly context-sensitive.\n\n2.2.5 Results of Transformer Models for Automated Essay Scoring Tasks\n\nTable 3 shows results of current research studies using transformers for AES. Rodriguez and colleagues\n[50] use the same Kaggle dataset as the studies shown in Table 2 and compare two transformer-based models,\nBERT [4] and XLNet [51], with a variety of other models. In particular, they show that the transformer-based\napproaches yield results comparable to that of a model combined of LSTM and CNN (see Table 2 above).\n\nMayfield and Black [34] also analyzed the same reference dataset but considered only five of the eight\nquestions in their analyses arguing that the remaining questions with 10 or more scoring classes each are not\nrepresentative of overall performance and can skew the reported results. They applied different transformer-\nbased models with DistilBERT [52] yielding the highest Kappa value. However, for the case of the selected five\nquestions from the dataset the n-gram model based on the BOW approach yields a comparable result, while the\ntransformer-based approach results in a large increase in needed computational power.\n\nTable 3. Recent Studies on Transformer Models for Automated Essay Scoring Tasks\n\nStudy Task Data Model Kappa\nRodriguez Scoring essay 12,978 essays with BERT 75\net al. (2019) answers to 8 a length of 150 to XLNet 75\n\ndifferent 550 words\n\nquestions, some\n\nof which depend\n\nupon source\n\ninformation\nMayfield, & Scoring essay 1,800 essays for N-Gram 76\nBlack answers to 5 each of the five DistiIBERT 75\n(2020) different questions, each\n\nquestions, some with a length of\n\nof which depend —_150 to 350 words\nupon source\ninformation\n\n3. Data\n\nTo investigate the potential of transformer-based models for AES, we used a dataset from the study\n“Domain-Specific Problem-Solving Competence for Industrial Clerks (“DomPL-IK”) conducted in 2014 with\n780 vocational and educational training students [53]. The trainees, enrolled in their second or third year of a\nthree-year commercial apprenticeship program learning, were asked to solve three complex domain-specific\nproblems within the business domain “controlling” and to communicate their decision via email.\n\nIn scenario 1, students had to conduct a deviation analysis to compute target costs and the variances\nbetween target and actual costs within a spreadsheet. Then they were asked to integrate the computed\nquantitative results with qualitative characteristics provided within information-enriched documents. Finally,\nthe participant needed to communicate possible consequences to a fictitious colleague via email [53]. Similarly,\nin scenario 2, participants selected a supplier based on a benefit analysis considering quantitative and\nqualitative characteristics. And in scenario 3, students needed to decide between in-house production and an\nexternal supplier. Distributed over these three business scenarios, the 780 apprentices submitted 2088 non-\n\nAn exception are models like the GPT-3, which have been successfully applied to a variety of tasks without further fine-tuning\nbut by designing appropriate prompts only.\nFor an introduction on transformers and transfer learning we recommend: https://huggingface.co/course/chapter1/4",
    "type": "text"
  },
  {
    "page": 8,
    "bbox": null,
    "text": "empty email answers in German, including on average 61.9 words (SD = 41.2). The maximum length was 512\nwords. However, most essays had a length of between 32 and 84 words (Mdn = 54, see Figure 1).\n\n30 60) 90) 420\nword count\n\nFigure 1. Boxplot representing the typical\nnumber of words per essay (outliers adjusted).\n\nFurthermore, as in many classification problems, the dataset is very unbalanced: The majority class “polite”\nincludes 92.5% of the cases (n = 1,942) and the minority class “impolite” only 7.5% (n = 146), which will be\nimportant to take care of during model training and evaluation.\n\nConsidering the human rating, the submitted student emails have been manually scored by trained experts\non previously defined scoring rubrics. For the given analyses, we selected an item which classifies whether the\nresponse email addressed to the supervisor was written in a polite form, or not. In the study’s competence\nmodel this item is part of the dimension measuring “Communicating Decisions Appropriately”. We assume\nthat this classification is very much task- and domain-independent, and it should therefore be possible to\ntransfer and adapt a corresponding AES to new, previously unseen texts. Emails that contain disrespectful,\nsarcastic, or rude phrases [54] are scored, or labeled (the more commonly used expression in ML), as “impolite”,\nwhereas emails with a courteous tone throughout are labeled “polite”. For further details regarding the scoring\nprocess please refer to Seifried and colleagues [55] and Brandt, Rausch, and Kégler [56].\n\n4. Method\n\nIn the following we describe in detail how the regression-based and the transformer-based classification\napproaches are implemented. The code is implemented in Python since the main packages to train machine\nlearning models are provided in Python, and guidelines and code examples for the latest models are typically\nalso only provided in Python. All Python code used for the analyses presented in this paper is provided in a\nGitHub repository, which is linked in the appendix.\n\nFor readers more familiar with the R infrastructure, we additionally included R markdown scripts in the\nrepository, which allow the Python code to be run using the common RStudio environment.\n\n4.1 Basic Data Preparation Used in Both Approaches\n\nThe basic dataset includes the two columns “text” and “label”, where the column text includes the emails\nas plain text (no html) with line feeds included as “\\n”; the column label includes the integers “0” or “1”\nrepresenting “impolite” or “polite”. The data import depends on the local or cloud infrastructure that is used\nfor the computation and information of the different forms are given in the GitHub repository. The first\nimportant step after importing the data is to randomly split it into training (70%) and test (30%) data. A\nvalidation dataset is neither in the regression-based nor in the transformer-based approach model needed since\nwe do not perform any hyper-parameter tuning of the models.\n\nFor the implementation of the split, we use the function train_test_split() from the Scikit-learn\npackage [57]. We use a seed for the random state of the function to assure that the random split stays the same\nin any repetition.\n\nfrom sklearn.model_ selection import train_test_split\ntrain_text_series, test_text_series, train_label_ series,\ntest_label_ series = train_test_split(data[\"text\"], data[\"label\"],\ntest_size = 0.30, random_state = 42)\n\nThe train_test_split() function returns a Pandas Series object. Since the tokenizer, which will be applied in\nthe next step expects a list object, we convert the data type correspondingly by using the function to_list ():",
    "type": "question"
  },
  {
    "page": 9,
    "bbox": null,
    "text": "train_text = train_text_series.to_list()\ntest_text = test_text_series.to_list()\ntrain_label = train_label_ series.to_list()\ntest_label = test_label series.to_list()\n\n4.2 Regression Model Estimation\n\nFor any machine learning task, it is good common practice to at first consider the results of a baseline model\nand then consider the benefits of applying a more sophisticated model. Regression analysis is a standard\napproach for supervised classification tasks [58] and frequently used in the field of AES [59,60].\n\nBefore conducting the actual regression analysis, it is necessary to first tokenize the text data given in the\nresponses and then extract meaningful features from them. The tokenization is done using a BOW approach\nand includes the following two steps:\n\n(1) removing stopwords, by using the stopword list for German from the NLTK library [61], and\n(2) stemming of all remaining words, by using the Porter Stemmer from the NLTK library [61].\nBoth steps are implemented in the function process_mail() included in the file utils. py, which is part of the\nlinked GitHub repository.\nTo extract the features for each response also two steps are conducted:\n(1) creating a frequency dictionary, with the information on how often a word was used in a polite or\nimpolite response, and\n(2) computing for each response a sum score for politeness and for impoliteness, based on the words\nincluded in the responses and their values in the frequency dictionary.\nFor the training data, the corresponding code looks as follows:\n\n# Create frequency dictionary\nfreqs = build freqs(train_text, train_label)\n\n# Extract features\ntrain_features = np.zeros((len(train_text), 2))\nfor i in range(len(train_text)):\ntrain_features[i, :]= extract_features(train_text[i], freqs)\n\nWith the aforementioned process_mail() function included in the build_freqs() function. After this,\nthe data frame train_features includes the respective sum scores for each of the responses. In the same way\nthe test data is preprocessed.\n\nIn the next step, we estimate the logistic regression model by using the function LogisticRegresion()\nfrom the Scikit-learn package [57].\n\nfrom sklearn.linear_model import LogisticRegression\nlog model = LogisticRegression(class weight= 'balanced').fit(train_features,\ntrain_label)\n\nClassification algorithms work best with balanced data sets. That is, when the number of cases in each class\nis equal [62]. For unbalanced datasets as the one considered in this paper, it is crucial to take care of this already\nduring training, to not get trivial results. The LogisticRegression() function therefore allows to specify an\nargument weight, which yields that each response gets a weight according to the class it belongs to, where\nresponses from smaller classes get higher weights and responses from larger classes smaller ones.\n\nFinally, we evaluate the results of the logistics regression by computing the confusion matrix, the accuracy,\nthe F1 score [63], the ROC AUC [64], and Cohen’s Kappa [65] for the predictions from the test data. While\naccuracy and Cohen’s Kappa are common measures in the social sciences to consider the extent of agreement\nbetween two ratings, the F1 score and the ROC AUC score are common measures in machine learning. By\n\n8 Regression Analysis can also be considered as a simple ML approach, as explained by Andrew Ng in his famous “Machine\nLearning” course:\nhttps://www.youtube.com/watch?v=kHwIB _j7Hkc&list=PLLssT5z_ DsK-h9VYZkOkYNWcltqhIRJLN&index=4",
    "type": "text"
  },
  {
    "page": 10,
    "bbox": null,
    "text": "10\n\ncomputing all measures, we want to consider their differences and communalities. The following Python code,\nyields the computation and export of the measures:\n\nfrom sklearn import metrics\n\nprint(\"Confusion Matrix:\\n\", metrics.confusion_matrix(test_label,\nlog_model.predict(test_features) ) )\n\nprint(\"Mean Accuracy:\\n\", log model.score(test_features, test_label) )\n\nprint(\"F1 Score:\\n\", metrics.f1_score(test_label, log model.predict(test_features)))\n\nprint(\"ROC AUC:\\n\", metrics.roc_auc_score(test_label,\nlog_model.predict(test_features) ) )\n\nprint(\"Cohen's Kappa:\\n\", metrics.cohen_kappa_score(test_label,\nlog_model.predict(test_features) ) )\n\n4.3 Transformer-Based Classification\n\nJust as in the regression-based approach, the first step for the classification based on a transformer model\nis to tokenize the text data. The form of the tokenization is strictly bound to the pretrained transformer that will\nbe used. We use two different pretrained models from the Hugging Face library [66] for our AES task and\ncorrespondingly use two different tokenizer functions provided together with these models via the Hugging\nFace library. The two models, or checkpoints, we use are “bert-base-german-cased” and “deepset/gbert-base”,\nwhich were released in June 2019 and October 2020, respectively [67]. A main difference between the two\nmodels is the size of the dataset used for the pretraining. While the first model is trained on about 12 GB of text\ndata, the newer one is trained on 163 GB of text.\n\nThe Python code to tokenize text for the checkpoint “deepset/gbert-base” is as follows:\n\nfrom transformers import AutoTokenizer\n\ncheckpoint = \"deepset/gbert-base”\ntokenizer = AutoTokenizer.from_pretrained(checkpoint )\n\ntrain_encodings = dict(tokenizer(train_text, padding=True, truncation=True,\nreturn_tensors='np'))\n\nThe class AutoTokenizer from the transformer package includes a function from_pretrained(), which\nallows to instantiate a tokenizer for the selected checkpoint. This tokenizer is then applied on the text data (here\nfor the training data). It splits all texts into subwords according to the WordPiece tokenization method [21],\npads or truncates the resulting vectors to the length of the chosen model’s context window (i.e., the maximum\nnumber of tokens), and assigns an integer ID to each text token. The resulting data is then returned in form of\na dictionary. Depending on the model, it might also include some additional advanced methods, which we do\nnot need in this case though but only the plain dictionary. Therefore, we additionally apply the function dict ()\nto receive a standard Python dictionary.\n\nTokenizing text for the checkpoint “bert-base-german-cased” is done by simply changing the text string\nproviding the name of the checkpoint correspondingly.\n\nAs mentioned in the previous section on the regression analysis, it is crucial to consider the unbalanced\nnumber of elements in the classes. For the fine-tuning of the transformer model, we must explicitly compute\nand define weights for the two classes. In the two lines of code below we, at first, compute how often each label\nis present in the training dataset and, then, define a dictionary, which assigns a weight to each label in that way\nthat it compensated for the differences in their frequencies.\n\nunique, counts = numpy.unique(train_label, return_counts=True)\nclass weight = {@: counts[1]/counts[@], 1: 1.0}\n\nIn general, there are various other ways to tackle the issue of inbalances data (see, for example, He and Garcia [68]). However,\nfor comparibility reasons, we chose to use the same approach here as in the regression analysis.",
    "type": "text"
  },
  {
    "page": 11,
    "bbox": null,
    "text": "11\n\nThe next step is to define the model that will be fine-tuned. We conduct the fine-tuning based on the Keras\nframework [69]. The definition of the model includes setting various hyperparameters, which are then typically\noptimized via numerous optimization runs and by evaluating the results with on a validation dataset. However,\nfor our analyses we refrain from optimizing the model’s hyperparameters and use the standard setting\nrecommended in the Hugging Face course [19]. Overall, we define the batch size, the number of epochs, the\nlearning rate, the optimizer, the model architecture, and the loss function, and these definitions are then\ncompiled into a single model. As described in the commented code below, the learning rate is defined via a\nscheduler that is adjusted after each training step, where the change in the learning rate depends on the given\nbatch size and the number of epochs as well as on the size of the training dataset. To obtain the definition of the\nmodel architecture and the pretrained weights, we use the function from_pretrained() from the class\nTFAutoModelForSequenceClassification of the transformers library. By choosing this class, the Hugging\nface library provides a TensorFlow (i.e., Keras) version of the provided checkpoint and adds a classification\nhead to it. The argument num_labels=2 further indicates that the classification head is to be defined for two\nclasses. The code for the model definition then looks as follows (the initial import statements have been omitted\nhere):\n\n# Definition of batch size and number of epochs\nbatch_size = 8\nnum_epochs = 3\n\n# Definition of the learning rate scheduler\n\n# The number of training steps is the number of samples in the dataset, divided by\nthe batch size then multiplied by the total number of epochs\n\nnum_train_steps = (len(train_label) // batch_size) * num_epochs\n\n1lr_scheduler = PolynomialDecay(initial_learning rate=5e-5, end_learning rate=0.,\ndecay_steps=num_train_steps)\n\n# Definition of the optimizer using the learning rate scheduler\nopt = Adam(learning rate=lr_scheduler)\n\n# Definition of the model architecture and initial weights\nmodel = TFAutoModelForSequenceClassification.from_pretrained( checkpoint,\nnum_labels=2)\n\n# Definition of the loss function\nloss = SparseCategoricalCrossentropy(from_logits=True)\n\n# Definition of the full model for training (or fine-tuning)\nmodel.compile(optimizer=opt, loss=loss, metrics=['accuracy'])\n\nAfter the model definition, the fine-tuning of the model is straightforward and conducted with the defined\nnumber of epochs, batch size, and weights:\n\nmodel. fit(train_encodings, np.array(train_label),\nclass_weight=class weight, batch_size=batch_size,\nepochs=num_epochs)\n\nAfter fine-tuning the model with the training data, we can then use the model to predict the labels for the\ntest data. This is done in two steps. In the first, we compute for all responses the probabilities of each label, and\nin the second, we select for each response the label with the highest probability as prediction. To get the\nprobabilities in the first step, we apply a softmax function to the predicted logits” that are returned as result\nfrom the fine-tuned model. The softmax function normalizes the logits and makes them interpretable as\nprobabilities.\n\n10 In machine learning the term logits usually refers to the raw predictions that a classification model generates and get passed to\nthe softmax function. It is not referring to the inverse of the sigmoid function here.",
    "type": "text"
  },
  {
    "page": 12,
    "bbox": null,
    "text": "11\n\nimport tensorflow as tf\ntest_pred_prob = tf.nn.softmax(model.predict(dict(test_encodings) )['logits'])\ntest_pred_class = np.argmax(test_pred_prob, axis=1)\n\nFinally, we compute the confusion matrix and the same four measures of agreement for the predicted test\nlabels, which is equivalent to the computation shown in the regression analysis above.\n\n5. Results\n\nThe most complete comparison of the agreements of the three considered models with the human ratings\nprovide the confusion matrices shown in Table 4.\n\nTable 4. Confusion Matrices\n\nActual \\ Predicted Impolite Polite\nRegression results for test data\n\nImpolite 31 15\n\nPolite 87 494\nGerman BERT (Jun. 2019) results for test data\n\nImpolite 29 17\n\nPolite 35 546\nGerman BERT (Oct. 2020) results for test data\n\nImpolite 32 14\n\nPolite 24 557\n\nHowever, for easier comparison, it is common to compute single scores for each model. Common scores\nare accuracy, Fl, ROC AUC, and Cohen’s Kappa, the results of which are shown in Table 5.\n\nTable 5. Comparison of Model Performances\n\nROC AUC Cohen’s\n\n1 A FI\n\nMode ccuracy Score Score Kappa\nLogistic 84 91 76 30\nRegression\nGerman BERT\n\n92 95 77 52\n(June 2019)\nGerman BERT\n\n94 97 82 59\n(Oct. 2020)\n\nThe accuracy and F1 score show values above .9 in five out of six cases, indicating a good performance on\nthe given AES task across all three models. However, as described above, the classes are largely imbalanced\nresulting in skewed results for the F1 score as well as for accuracy (see [70]). More informative measures are\ntherefore the ROC AUC score and Cohen’s Kappa.!! For both measures, the transformer-based approaches yield\nbetter values than the regression-based approach, and the larger transformer model yielded the best results for\nall measures. Considering the achieved values, Cohen suggests values between .4 and .6 as moderate\nagreement, values between .6 and .8 as substantial agreement, and values between .8 and 1 as almost perfect\nagreement [72]. The result of the large transformer model is for the given data and according to Cohen’s Kappa\ntherefore still slightly below a good performance. Considering the ROC AUC, Mandrekar [73] suggests to\nconsider values between .7 and .8 as acceptable, between .8 and .9 as excellent, and above .0 as outstanding.\nAccording to ROC AUC standards the performance of the large transformer model can therefore be considered\nas good.\n\nFor more details on the relationship of the ROC AUC and Cohen’s Kappa, we recommend [71].",
    "type": "text"
  },
  {
    "page": 13,
    "bbox": null,
    "text": "13\n\nBesides the overall measures on correctly classified responses, the transformer models provide a\nprobability estimate for each prediction. We therefore also analyzed how many of the predictions have\nestimated probabilities of 95% or more. Of the total 627 responses in the test data, 554 predictions, or 88.3%,\nhave a probability of 95% or more. From these 554 predictions a total of 11 responses are not classified in\naccordance with the human rating (six are coded as 1 by the machine, but as 0 by the human raters; 5 are coded\nas 0, but as 1 by the human raters). After carefully reviewing these responses, we found that all these responses\nwere in fact classified incorrectly by the human raters and were not coded in accordance with the coding\nguidelines.'?\n\n6. Discussion\n\nThe results have shown that for the humagiven task of scoring a text according to its politeness, particularly\nthe large transformer-based model reach reasonable results while the regression-based approach struggles to\nscore correctly, particularly for impolite emails. The benefit of applying transformer-based models for AES has\nrecently been investigated by several authors (see section 2.2.5). In this context, Mayfield and Black [34] argue\nthat although the increase in the quadratic weighted kappa is only about 5%, it comes at a large opportunity\ncost in computational resources. And the question of whether the gain in precision outweighs the computational\nburden that comes with fine-tuning large transformer models like BERT. At the same time, however, they stress\nthe benefits of transformer models when it comes to analyzing writing style, an area with “uncharted territory\nfor AES”. Scoring the politeness of a text, is very much like scoring the style of a text. In the given example with\nGerman text, it is crucial, for example, to know if formal or informal speech is used, information that would\ntypically be removed by stemming. Furthermore, it might be important where in the text a certain word is used,\nfor example in the greeting at the beginning or later in a different context. The BOW approach, on which the\nregression analysis is based on, does not consider word order though. For these reasons, it is not surprising that\nthe transformer-based approaches outperformed the regression-based approach for the given AES task.\n\nThe comparison of the two BERT models further shows the importance of the language model that was\ntrained using the BERT architecture. Using a BERT with the currently most up-to-date language model for\nGerman increased the accuracy scores significantly in comparison to the one that was state-of-the-art the year\nbefore. Kaplan and colleagues [36] have shown that larger transformer models based on more training data will\nalso continue to perform better in the future. It is therefore reasonable to assume that the transformer-based\napproach will continue to yield better results on AES in the future. The size of the models is increasingly\nconsidered a problem though, as mentioned above. Hernandez and Brown [74], however, have shown that the\namount of compute necessary for a state-of-the-art neural net in 2012 has decreased by a factor of 44x for a net\nwith the same performance in 2019, which even outperforms the Moore’s law rate of improvement in hardware\nefficiency [75]. This observation is, for example, in line with the work of Sanh and colleagues on DistilIBERT, in\nwhich they have shown that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its\nlanguage understanding capabilities and being 60% faster [52]. We therefore think that the efficiency of the\ntransformer-based models will further increase in the future and additionally that much smaller models with\nadequate language models for AES will be developed.\n\nAs we have seen in the results the scores given by the machine learning algorithms include errors.\nIntroducing a fully automated essay scoring is therefore often viewed with skepticism. However, the first step\nin getting reliably automated essay scores is having reliable labels for training, that is, reliable scores from\nhuman raters. Creating such labels is a very laborious and time-consuming task, and two main reasons are: (1)\nOften a large part of the rating is relatively easy and clear, but nevertheless they cost time and still allow for\ntypos in the actual coding process; and (2) for borderline cases between two scores, it is very hard to get a clear\nunderstanding of where to draw the line. Hence, two raters might have slightly different understandings how\nto interpret the scoring guidelines or a single rater might deviate in his coding due to the halo effect, where the\noverall impression of the participant’s answers biases the scoring [76]. Furthermore, misclassification can also\nbe caused by reasons such as, a lack of training, unclear coding guidelines, or a lack of motivation. To obtain\nreliable scores, it is therefore beneficial to have an answer scored not only by one human rater but by two and\nin case they deviate by maybe even three raters. However, such an approach is very expensive and, in most\nstudies, hardly feasible due to the resulting costs. Training a machine learning model, even a BERT model, is\n\n12\n\nFor data privacy reasons it is unfortunately not possible to share the responses.",
    "type": "text"
  },
  {
    "page": 14,
    "bbox": null,
    "text": "14\n\nvery cheap in comparison. Furthermore, they have the advantage of not only providing a prediction of the score\nbut also a probability estimate of its correctness. For the given example, we saw that the model based on the\nlatest German BERT model is correct on all scores that are estimated with a probability of 95% or more. For\n88.3% of the answers this was the case. Hence, with using the machine score for all answers with such high\nprobability estimates, the number of answers that needs to be scored by human raters could be reduced to the,\nin this case, 11.7% remaining answers. That is, even when assigning each of these answers to three human raters,\nthe costs are still less then when scoring all answers manually by just one person. Moreover, there is a high\nchance that the scoring will become more accurate, which then in turn leads to better training data and more\naccurate machine scores.\n\n7. Conclusion\n\nAbove, we discussed the benefits of applying a transformer-based model for AES. The most important\nfactor of how much benefit it provides compared to a BOW approach, will always be the nature of the task that\nis rated. For knowledge tests, it may well be that a BOW approach is sufficient and more efficient to determine\nwhether, for example, all relevant elements are mentioned. In the given case of measuring a communicative\nskill, we have shown that the difference in performance can be large, though. We think that the same will also\nhold for tasks, such as reasoning, in which the order of the words is of particular importance. However, we are\nnot aware of any comparable results yet for reasoning.\n\nA further important benefit of the transformer-based approach, or in more general of a language model-\nbased approach, is that we can expect future improvements to AES just by switching from one language model\nto another. In the same way, we can also easily switch to scoring a task in a different language by changing to\na corresponding language model or to a multi-lingual model, which then allows fine-tuning to the same task in\ndifferent languages simultaneously [77].\n\nConsidering the relationship between AES and scoring by human raters, we believe that while replacing\nhuman raters with a machine is problematic in many cases, an important role of AES can be to detect possible\nerrors by human raters and automatically score \"very easy\" texts to free up resources for hard-to-score texts.\nWe have shown that even without further hyper-parameter tuning but just by using the set of recommended\nstandard settings, it is already possible to obtain a model for such purposes. By providing detailed\ndocumentation, we hope to motivate others to use such models and improve human ratings.\n\nSupplementary Materials: All code used for the analyses shown in the paper and further additional code is available on\nGitHub under:\nhttps://github.com/LucaOffice/Publications/tree/main/Automatic_Essay_Scoring_Using_Transformer_Models.\n\nFunding: This research was funded by the German Federal Ministry of Education and Research, grant number\n01DB081119-01DB1123.\n\nInstitutional Review Board Statement: The study was approved by the Institutional Review Board (or Ethics Committee)\nof University of Bamberg, Germany.\n\nInformed Consent Statement: Informed consent was obtained from all subjects involved in the study.\n\nData Availability Statement: The dataset including the full texts of the response emails is unfortunately not publicly\navailable. Access to a dataset including the scored responses is available on request here: https://www.iqb.hu-\nberlin.de/fdz/studies/01DB1119_DomPL-IK/?doi=10.5159/IQB_DomPL-IK_v1\n\nAcknowledgments: We thank Andreas Rausch and Jiirgen Seifried for the extensive discussions we had on the results and\nfor their helpful critical comments on the earlier version of this manuscript. Finally, we appreciate financial support by the\n\nGerman Federal Ministry of Education and Research.\n\nConflicts of Interest: The authors declare no conflict of interest.",
    "type": "text"
  },
  {
    "page": 15,
    "bbox": null,
    "text": "References\n\n1. Chen, L.; Chen, P.; Lin, Z. Artificial Intelligence in Education: A Review. IEEE Access 2020, 8, 75264—\n75278, doi:10.1109/ACCESS.2020.2988510.\n\n2. Dikli, S. An Overview of Automated Scoring of Essays. The Journal of Technology, Learning and Assessment\n2006, 5, 36.\n\n3. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, L.; Polosukhin, I.\nAttention Is All You Need. arXiv:1706.03762 [cs] 2017.\n\n4. Devlin, J.; Chang, M.-W.,; Lee, K.; Toutanova, K. BERT: Pre-Training of Deep Bidirectional Transformers\nfor Language Understanding. arXiv:1810.04805 [cs] 2019.\n\n5. Page, E.B. The Imminence of... Grading Essays by Computer. The Phi Delta Kappan 1966, 47, 238-243.\n\n6. Page, E.B. Grading Essays by Computer: Progress Report. In Proceedings of the Proceedings of the\ninvitational Conference on Testing Problems; 1967.\n\n7. Page, E.B. The Use of the Computer in Analyzing Student Essays. International review of education 1968,\n210-225.\n\n8. Chung, G.K.; O’Neil Jr, H.F. Methodological Approaches to Online Scoring of Essays. 1997.\n\n9. Hearst, M.A. The Debate on Automated Essay Grading. IEEE Intell. Syst. Their Appl. 2000, 15, 22-37,\ndoi:10.1109/5254.889104.\n\n10. Chen, Y.-Y.; Liu, C.-L.; Chang, T.-H.; Lee, C.-H. An Unsupervised Automated Essay Scoring System.\nIEEE Intell. Syst. 2010, 5396298, doi:10.1109/MIS.2010.3.\n\n11. Leacock, C.; Chodorow, M. C-Rater: Automated Scoring of Short-Answer Questions. 2003, 18.\n\n12. Mahana, M.; Johns, M.; Apte, A. Automated Essay Grading Using Machine Learning. Mach. Learn.\nSession, Stanford University. 2012, 5.\n\n13. Jurafsky, D.; Martin, J.H. Speech and language processing. In Speech and language processing; Pearson\nLondon, 2020; Vol. 3.\n\n14. Zhang, Y.; Jin, R.; Zhou, Z.-H. Understanding Bag-of-Words Model: A Statistical Framework. Int. J.\nMach. Learn. & Cyber. 2010, 1, 43-52, doi:10.1007/s13042-010-0001-0.\n\n15. Burstein, J.; Kulcich, K.; Wolff, S.; Lut, C.; Chodorowl, M. Enriching Automated Essay Scoring Using\nDiscourse Marking. 2001, 11.\n\n16. Foltz, P.W.; Gilliam, S.; Kendall, S. Supporting Content-Based Feedback in On-Line Writing Evaluation\nwith LSA. Interactive Learning Environments 2000, 8, 111-127, doi:10.1076/1049-4820(200008)8:2;1-\nB;FT111.\n\n17. Foltz, P.W.; Laham, D.; Landauer, T.K. The Intelligent Essay Assessor: Applications to Educational\nTechnology. Interactive Multimedia Electronic Journal of Computer-Enhanced Learning, 1999, 1, 939-944.\n\n18. Application Checkpointing. Wikipedia 2021.\n\n19. Transformer Models - Hugging Face Course Available online: https://huggingface.co/course/ (accessed\non 24 August 2021).\n\n20. Andersen, N.; Zehner, F. ShinyReCoR: A Shiny Application for Automatically Coding Text Responses\nUsing R. Psych 2021, 3, 422-446, doi:10.3390/psych3030030.\n\n21. Wu, Y.; Schuster, M.; Chen, Z.; Le, Q.V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.;\n\nMacherey, K.; et al. Google’s Neural Machine Translation System: Bridging the Gap between Human\nand Machine Translation. arXiv:1609.08144 [cs] 2016.",
    "type": "text"
  },
  {
    "page": 16,
    "bbox": null,
    "text": "22.\n\n23.\n\n24.\n\n25.\n\n26.\n\n27.\n\n28.\n\n29.\n\n30.\n\n31.\n\n32.\n\n33.\n\n34.\n\n35.\n\n36.\n\n37.\n\n38.\n\n39.\n\nSennrich, R.; Haddow, B.; Birch, A. Neural Machine Translation of Rare Words with Subword Units.\narXiv:1508.07909 [cs] 2016.\n\nBrown, T.B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.;\nSastry, G.; Askell, A.; et al. Language Models Are Few-Shot Learners. arXiv:2005.14165 [cs] 2020.\nKudo, T.; Richardson, J. SentencePiece: A Simple and Language Independent Subword Tokenizer and\nDetokenizer for Neural Text Processing. arXiv:1808.06226 [cs] 2018.\n\nKudo, T. Subword Regularization: Improving Neural Network Translation Models with Multiple\nSubword Candidates. arXiv:1804.10959 [cs] 2018.\n\nPennington, J.; Socher, R.; Manning, C.D. GloVe: Global Vectors for Word Representation. In\nProceedings of the Empirical Methods in Natural Language Processing (EMNLP); 2014; pp. 1532-1543.\nPapers with Code Papers with Code - Language Modelling Available online:\nhttps://paperswithcode.com/task/language-modelling (accessed on 28 April 2021).\n\nCho, K.; van Merrienboer, B.; Gulcehre, C.; Bahdanau, D.; Bougares, F.; Schwenk, H.; Bengio, Y.\nLearning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation.\narXiv:1406.1078 [cs, stat] 2014.\n\nHochreiter, S.; Schmidhuber, J. Long Short-Term Memory. Neural computation 1997, 9, 1735-80,\ndoi:10.1162/neco.1997.9.8.1735.\n\nPeters, M.E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; Zettlemoyer, L. Deep\nContextualized Word Representations. arXiv:1802.05365 [cs] 2018.\n\nHochreiter, S. The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem\nSolutions. Int. J. Unc. Fuzz. Knowl. Based Syst. 1998, 06, 107-116, doi:10.1142/S0218488598000094.\nTaghipour, K.; Ng, H.T. A Neural Approach to Automated Essay Scoring. In Proceedings of the\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing;\nAssociation for Computational Linguistics: Austin, Texas, 2016; pp. 1882-1891.\n\nAlikaniotis, D.; Yannakoudakis, H.; Rei, M. Automatic Text Scoring Using Neural Networks. Proceedings\nof the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) 2016,\n715-725, doi:10.18653/v1/P16-1068.\n\nMayfield, E.; Black, A.W. Should You Fine-Tune BERT for Automated Essay Scoring? In Proceedings of\nthe Proceedings of the Fifteenth Workshop on Innovative Use of NLP for Building Educational\nApplications; Association for Computational Linguistics: Seattle, WA, USA — Online, 2020; pp. 151-\n162.\n\nBahdanau, D.; Cho, K.; Bengio, Y. Neural Machine Translation by Jointly Learning to Align and\nTranslate. arXiv:1409.0473 [cs, stat] 2016.\n\nKaplan, J.; McCandlish, S.; Henighan, T.; Brown, T.B.; Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.;\nAmodei, D. Scaling Laws for Neural Language Models. arXiv:2001.08361 [cs, stat] 2020.\n\nHuang, X.; Khetan, A.; Cvitkovic, M.; Karnin, Z. TabTransformer: Tabular Data Modeling Using\nContextual Embeddings. arXiv:2012.06678 [cs] 2020.\n\nWu, N.; Green, B.; Ben, X.; O’Banion, S. Deep Transformer Models for Time Series Forecasting: The\nInfluenza Prevalence Case. arXiv:2001.08317 [cs, stat] 2020.\n\nHu, R.; Singh, A. UniT: Multimodal Multitask Learning with a Unified Transformer. arXiv:2102.10772\n[cs] 2021.",
    "type": "text"
  },
  {
    "page": 17,
    "bbox": null,
    "text": "40.\n\nAl.\n\n42.\n\n43.\n\n44.\n\n45.\n\n46.\n\n47.\n\n48.\n\n49.\n\n50.\n\n51.\n\n52.\n\n53.\n\n54.\n\n55.\n\n56.\n\n57.\n\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.;\nMinderer, M.; Heigold, G.; Gelly, S.; et al. An Image Is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. arXiv:2010.11929 [cs] 2021.\n\nGalanis, N.-I.; Vafiadis, P.; Mirzaev, K.-G.; Papakostas, G.A. Machine Learning Meets Natural Language\nProcessing -- The Story so Far. arXiv:2104.10213 [cs] 2021.\n\nPapers with Code - Browse the State-of-the-Art in Machine Learning Available online:\nhttps://paperswithcode.com/sota (accessed on 2 September 2021).\n\nSharir, O.; Peleg, B.; Shoham, Y. The Cost of Training NLP Models: A Concise Overview.\narXiv:2004.08900 [cs] 2020.\n\nStrubell, E.; Ganesh, A.; McCallum, A. Energy and Policy Considerations for Deep Learning in NLP.\narXiv:1906.02243 [cs] 2019.\n\nLiu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; Stoyanov, V.\nRoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs] 2019.\n\nLan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R. ALBERT: A Lite BERT for Self-\nSupervised Learning of Language Representations. arXiv:1909.11942 [cs] 2020.\n\nUto, M. A Review of Deep-Neural Automated Essay Scoring Models. Behaviormetrika 2021, 48, 459-484,\ndoi:10.1007/s41237-021-00142-y.\n\nPuri, R.; Catanzaro, B. Zero-Shot Text Classification With Generative Language Models.\narXiv:1912.10165 [cs] 2019.\n\nHoward, J.; Ruder, S. Universal Language Model Fine-Tuning for Text Classification. arXiv:1801.06146\n[cs, stat] 2018.\n\nRodriguez, P.U.; Jafari, A.; Ormerod, C.M. Language Models and Automated Essay Scoring.\narXiv:1909.09482 [cs, stat] 2019.\n\nYang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; Le, Q.V. XLNet: Generalized Autoregressive\nPretraining for Language Understanding. arXiv:1906.08237 [cs] 2020.\n\nSanh, V.; Debut, L.; Chaumond, J.; Wolf, T. DistiIBERT, a Distilled Version of BERT: Smaller, Faster,\nCheaper and Lighter. arXiv:1910.01108 [cs] 2020.\n\nRausch, A.; Seifried, J.; Wuttke, E.; Kégler, K.; Brandt, S. Reliability and Validity of a Computer-Based\nAssessment of Cognitive and Non-Cognitive Facets of Problem-Solving Competence in the Business\nDomain. Empirical research in vocational education and training 2016, 8, 9.\n\nSembill, D.; Rausch, A.; Wuttke, E.; Seifried, J.; Wolf, K.D.; Martens, T.; Brandt, S. Domain specific\nProblem-solving Skills of Prospective Industrial Clerks (DomPL-IK)Modellierung und Messung\ndomdanenspezifischer Problemlésekompetenz bei Industriekaufleuten (DomPL-IK) 2016.\n\nSeifried, J.; Brandt, S.; Kogler, K.; Rausch, A. The Computer-Based Assessment of Domain-Specific\nProblem-Solving Competence—A Three-Step Scoring Procedure. Cogent Education 2020, 7, 1719571,\ndoi:10.1080/2331186X.2020.1719571.\n\nBrandt, S.; Rausch, A.; Kogler, K. A Scoring Procedure for Complex Assessments Focusing on Validity\nand Appropriate Reliability. 2016, 20.\n\nPedregosa, F.; Varoquaux, G.; Gramfort, A.; Michel, V.; Thirion, B.; Grisel, O.; Blondel, M.; Prettenhofer,\nP.; Weiss, R.; Dubourg, V.; et al. Scikit-Learn: Machine Learning in Python. Journal of Machine Learning\nResearch 2011, 12, 2825-2830.",
    "type": "text"
  },
  {
    "page": 18,
    "bbox": null,
    "text": "58.\n\n59.\n\n60.\n\n61.\n\n62.\n\n63.\n\n64.\n\n65.\n\n66.\n\n67.\n\n68.\n\n69.\n70.\n\n71.\n\n72.\n73.\n\n74.\n\n75.\n\n76.\n\n77.\n\nGéron, A. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and\nTechniques to Build Intelligent Systems; O’Reilly Media, 2019; ISBN 1-4920-3261-1.\n\nAttali, Y.; Burstein, J. Automated Essay Scoring with E-Rater® V. 2. The Journal of Technology, Learning\nand Assessment 2006, 4.\n\nHaberman, S.J.; Sinharay, S. The Application of the Cumulative Logistic Regression Model to\nAutomated Essay Scoring. Journal of Educational and Behavioral Statistics 2010, 35, 586-602.\n\nBird, S.; Loper, E.; Klein, E. Natural Language Processing with Python; O’Reilly Media Inc., 2009;\n\nChawla, N.V. Data Mining for Imbalanced Datasets: An Overview. In Data Mining and Knowledge\nDiscovery Handbook; Maimon, O., Rokach, L., Eds.; Springer-Verlag: New York, 2005; pp. 853-867 ISBN\n978-0-387-24435-8.\n\nChinchor, N. MUC-4 Evaluation Metrics. In Proceedings of the Proceedings of the 4th conference on\nMessage understanding; Association for Computational Linguistics: USA, June 16 1992; pp. 22-29.\nHanley, J.A.; McNeil, B.J. The Meaning and Use of the Area under a Receiver Operating Characteristic\n(ROC) Curve. Radiology 1982, 143, 29-36.\n\nCohen, J. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement\n1960, 20, 37-46, doi:10.1177/001316446002000104.\n\nWolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P.; Rault, T.; Louf, R.;\nFuntowicz, M.; et al. HuggingFace’s Transformers: State-of-the-Art Natural Language Processing.\narXiv:1910.03771 [cs] 2020.\n\nChan, B.; Schweter, S.; Moller, T. German’s Next Language Model. arXiv:2010.10906 [cs] 2020.\n\nHe, H.; Garcia, E.A. Learning from Imbalanced Data. IEEE Transactions on Knowledge and Data\nEngineering 2009, 21, 1263-1284, doi:10.1109/TKDE.2008.239.\n\nChollet, F.; others Keras 2015.\n\nPowers, D.M.W. Evaluation: From Precision, Recall and F-Measure to ROC, Informedness, Markedness\nand Correlation. arXiv:2010.16061 [cs, stat] 2020.\n\nBen-David, A. About the Relationship between ROC Curves and Cohen’s Kappa. Engineering\nApplications of Artificial Intelligence 2008, 21, 874-882, doi:10.1016/j.engappai.2007.09.009.\n\nMcHugh, M.L. Interrater Reliability: The Kappa Statistic. Biochem Med (Zagreb) 2012, 22, 276-282.\nMandrekar, J.N. Receiver Operating Characteristic Curve in Diagnostic Test Assessment. Journal of\nThoracic Oncology 2010, 5, 1315-1316, doi:10.1097/JTO.0b013e3181ec173d.\n\nHernandez, D.; Brown, T.B. Measuring the Algorithmic Efficiency of Neural Networks.\narXiv:2005.04305 [cs, stat] 2020.\n\nMoore, G.E. Cramming More Components onto Integrated Circuits. Electronics 1965, 38, 6.\n\nEngelhard, G. Examining Rater Errors in the Assessment of Written Composition With a Many-Faceted\nRasch Model. Journal of Educational Measurement 1994, 31, 93-112, doi:10.1111/j.1745-\n3984.1994.tb00436.x.\n\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V.; Wenzek, G.; Guzman, F.; Grave, E.; Ott, M.;\nZettlemoyer, L.; Stoyanov, V. Unsupervised Cross-Lingual Representation Learning at Scale. CoRR\n2019, abs/1911.02116.",
    "type": "text"
  }
]